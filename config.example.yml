# Talk2Metadata Configuration Example
# Copy this file to config.yml and customize as needed

# Run ID for organizing multiple runs (optional)
# If set, all data will be organized under {output_dir}/{run_id}/
# You can use simple format: run_id: "wamex_run"
# Or extended format with custom output directory:
# run_id: null # Optional run ID (e.g., "v1", "experiment_1")
# run:  # Extended format (optional)
#   id: wamex_run  # Run ID
#   output_dir: "./data"  # Output base directory (default: "./data")
#   # Custom output directories (optional, defaults to {output_dir}/{run_id}/{subdir})
#   # metadata_dir: "./custom/path/metadata"
#   # processed_dir: "./custom/path/processed"
#   # indexes_dir: "./custom/path/indexes"
#   # qa_dir: "./custom/path/qa"
#   # benchmark_dir: "./custom/path/benchmark"
#   # db_dir: "./custom/path/db"

# Schema detection settings
# Note: If run_id is set, processed/indexes/metadata paths are automatically
#       constructed as data/{run_id}/{subdir}
schema:
  fk_detection:
    use_heuristics: true
    inclusion_tolerance: 0.1  # Allow 10% mismatch in FK relationships
    min_coverage: 0.9          # Require 90% coverage for FK inference

# Mode configuration
# Each mode has its own indexer and retriever settings
modes:
  # Mode: record_embedding
  record_embedding:
    indexer:
      model_name: "sentence-transformers/all-MiniLM-L6-v2"  # Fast and accurate
      # Alternative models:
      # - "sentence-transformers/all-mpnet-base-v2"  # Higher quality, slower
      # - "sentence-transformers/multi-qa-MiniLM-L6-cos-v1"  # Optimized for QA
      device: null  # null for auto-detect, or 'cpu', 'cuda:0', etc.
      batch_size: 32
      normalize: true
    retriever:
      top_k: 5
      similarity_metric: "cosine"
      per_table_top_k: 5  # Results per table before voting
      use_reranking: false  # Requires cross-encoder (optional)

  # Future modes can be added here:
  # future_mode_2:
  #   indexer:
  #     ...
  #   retriever:
  #     ...

  # Global mode settings
  active: "record_embedding"  # Active mode to use for indexing and retrieval
  compare:  # Comparison mode settings
    enabled: false  # Enable comparison mode (runs all modes and compares)
    modes: []  # List of modes to compare (empty = all enabled modes)
    # Example: ["record_embedding", "future_mode_2"]

# Search configuration
search:
  output_format: "text"  # Output format: "text" or "json"
  show_score: false  # Show similarity scores in results

# Ingest configuration (optional, can be overridden by CLI arguments)
ingest:
  target_table: null  # Target table name (e.g., "orders")
  data_type: null     # Source type: "csv", "database", or "db"
  source_path: null  # Path to CSV directory or database connection string

# Evaluation configuration
evaluation:
  # Default top_k for evaluation (number of results to retrieve per query)
  top_k: 10
  
  # Output format for display: "text" or "json"
  output_format: "text"
  
  # Save format for results: "json", "txt", or "both"
  save_format: "both"
  
  # Auto-save results to benchmark directory (set to false to disable)
  auto_save: true
  
  # Evaluate all enabled modes by default (set to false to evaluate only active mode)
  evaluate_all_modes: false

# Agent configuration (optional, future feature)
agent:
  enabled: false
  provider: "openai"  # openai, anthropic, etc.
  model: "gpt-4o-mini"
  temperature: 0.0
